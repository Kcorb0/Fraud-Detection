{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0a6983c",
   "metadata": {},
   "source": [
    "# Assignment 2: Fraud Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355a1162",
   "metadata": {},
   "source": [
    "Author: Josh NM Blackmore <br>\n",
    "StID: 201776628"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded8cbe8",
   "metadata": {},
   "source": [
    "## Case Study\n",
    "\n",
    "<p>An insurance company plans to utilise their historic insurance fraud dataset to predict the likelihood or the level of risk a customer poses. You can find the dataset above. Referring genuine claims cause customer stress and directly leads to customer loss, costing the company money (assume that any referred non-fraud case will lead to losing that customer). While obviously, fraud claims cost the company as well. Their main requirement is to use an unbiased predictive model capable of flagging and referring potential fraud cases for further investigation with a balanced error rate of 5%.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1c5a2a",
   "metadata": {},
   "source": [
    "## 1 Aims, Objectives & Plan *(Revisit Section)*\n",
    "### 1.1 Aims & Objectives\n",
    "This projects primary objective is to analyse a medium sized dataset containing insurance claims from an insurance company with the goal of identifying potential fraudulant claims. The project requires a minumum of two techniques to provide predictions or valuable insights.\n",
    "\n",
    "Provided datasets will be pre-processed with various techniques and the reasoning behind certain decisions. \n",
    "\n",
    "Secondly, a technical report which consists of a narration around the analysis conducted for the project. This will be a jupyter notebook document containing the key procedures taken for each step such as justifying choices made in pre-processing, the models solutions, various visualisations of the analysis and testing with performance metrics such as F1, recall, confusion matrix etc.\n",
    "\n",
    "### 1.2 Plan\n",
    "- Gantt Chart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fb4099",
   "metadata": {},
   "source": [
    "## 2 Understanding the Case Study *(Revisit Section)*\n",
    "### 2.1 Case Study Analysis\n",
    "In this section I will present my understanding of the case study with the 4 critical points found in the case description.\n",
    "#### 2.1.1 Predicting a Customers Level of Risk\n",
    "Each row of the datasets provide information for an individual customers claim with labels depicting if the claim was fraudulant or legitimate. The client needs to know what level of risk future clients pose for future cases, using historical data.\n",
    "\n",
    "#### 2.1.2 Cost of Fraudulant Cases for Customers and Company\n",
    "Cases provided contain various cost factors such as property claim, injury claims etc. Part of the case study specifies the need to highlight the cost risk for fraudulant cases company and customer alike, the case study specifies the risk of losing customers due to cost. \n",
    "\n",
    "#### 2.1.3 Unbiased Prediction Model to Flag Fraudulant Cases\n",
    "The case study specifies the need of unbiased predictions. This can be acheived in pre-processing ensuring the training/test data is well balanced with fair distributions. Techniques such as regularization and post-processing techniques such as equalizing false positives and false negatives.\n",
    "\n",
    "#### 2.1.4 Accuracy and Error Rate of the Models\n",
    "Accuracy measures are needed to ensure the model is making correct predictions/accurate insights, such as correctly classified instances. The error rate will also be calculated to complement accuracy. The case study specifies the client is expecting a balanced error rate of around 5%.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc443b8",
   "metadata": {},
   "source": [
    "## 3 Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f078a9e9",
   "metadata": {},
   "source": [
    "#### Key Observations\n",
    "- No duplicate customerIDs.\n",
    "- 13 categorical values, 6 numerical. However there are some cases where missing values in numerical values have \"?\" or \"MISSINGVALUE\" which is causing the feature to register as non numerical.\n",
    "- Data window is from 2015-01-01 to 2015-03-14.\n",
    "- Binary class labels N=No, Y=Yes.\n",
    "- Witnesses contains 46 values with 'MISSINGVALUE' removing these rows makes the most sense as it is an insignificant portion of the data.\n",
    "- 'TypeOfCollission' contains 5162 values of '?'\n",
    "- 'PropertyDamage' contains 10459 values of '?'\n",
    "- 'PoliceReport' contains 9805 values of '?'\n",
    "- 'IncedentTime' has some negative values which does not make sense, these rows will be dropped."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c83408e",
   "metadata": {},
   "source": [
    "#### Importing Initial Permitted Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e54b7be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00297bf",
   "metadata": {},
   "source": [
    "#### Loading the Datasets into Pandas Dataframes for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33248900",
   "metadata": {},
   "outputs": [],
   "source": [
    "claims_csv = \"./Data/archive/TrainData/TrainData/Train_Claim.csv\"\n",
    "customers_withoutTarget_csv = \"./Data/archive/TrainData/TrainData/Traindata_withoutTarget.csv\"\n",
    "customers_withTarget_csv = \"./Data/archive/TrainData/TrainData/Traindata_with_Target.csv\"\n",
    "demographics_csv = \"./Data/archive/TrainData/TrainData/Train_Demographics.csv\"\n",
    "policy_csv = \"./Data/archive/TrainData/TrainData/Train_Policy.csv\"\n",
    "vehicle_csv = \"./Data/archive/TrainData/TrainData/Train_Vehicle.csv\"\n",
    "\n",
    "\n",
    "claims_df = pd.read_csv(claims_csv)\n",
    "customers_no_target_df = pd.read_csv(customers_withoutTarget_csv)\n",
    "customers_target_df = pd.read_csv(customers_withTarget_csv)\n",
    "demographics_df = pd.read_csv(demographics_csv)\n",
    "policy_df = pd.read_csv(policy_csv)\n",
    "vehicle_df = pd.read_csv(vehicle_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc4ec91",
   "metadata": {},
   "source": [
    "### 3.1 Preparing the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4bafb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "claims_with_labels = pd.merge(claims_df, customers_target_df, on='CustomerID')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97612948",
   "metadata": {},
   "source": [
    "### 3.2 Removing Synonymous and Noisy Atrributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cd1bd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aed0bcf4",
   "metadata": {},
   "source": [
    "### 3.3 Dealing with Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878bc740",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "027090ea",
   "metadata": {},
   "source": [
    "### 3.4 Dealing with duplicate values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da3d889",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "16d66af5",
   "metadata": {},
   "source": [
    "### 3.5 Rescaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f241f1a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72811a96",
   "metadata": {},
   "source": [
    "### 3.6 Dealing with Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1492ed8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a8eb677",
   "metadata": {},
   "source": [
    "### 3.7 Dealing with Collinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61946c60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
